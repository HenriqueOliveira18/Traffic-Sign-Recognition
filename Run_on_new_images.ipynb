{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import glob as glob\n",
    "import sys\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Environment Directory\n",
    "ENV_PATH = '/Users/henriqueoliveira18/Documents/Developer/traffic-sign-detection'\n",
    "\n",
    "# Go to the Environment Directory\n",
    "os.chdir(ENV_PATH)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone TensorFlow Model Garden\n",
    "if not os.path.exists('models'):\n",
    "    !git clone https://github.com/tensorflow/models.git\n",
    "else:\n",
    "    print('Models already cloned')\n",
    "\n",
    "# Clone Darkflow\n",
    "if not os.path.exists('darkflow'):\n",
    "    !git clone https://github.com/thtrieu/darkflow.git\n",
    "else:\n",
    "    print('Darkflow already cloned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install protobuf-compiler python-pil python-lxml python-tk\n",
    "!cd '/Users/henriqueoliveira18/Documents/Developer/traffic-sign-detection/models/research' && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python3 -m pip install . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if all the dependencies are installed\n",
    "%cd '/Users/henriqueoliveira18/Documents/Developer/traffic-sign-detection/models/research/object_detection/builders'\n",
    "!python3 model_builder_tf2_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd '/Users/henriqueoliveira18/Documents/Developer/traffic-sign-detection/models/research/object_detection'\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Loading the Model**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Selecting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    1: 'faster_rcnn_inception_resnet_v2_atrous',\n",
    "    2: 'faster_rcnn_resnet_101',\n",
    "    3: 'faster_rcnn_resnet50',\n",
    "    4: 'faster_rcnn_inception_v2',\n",
    "    5: 'rfcn_resnet101',\n",
    "    6: 'ssd_inception_v2',\n",
    "    7: 'ssd_mobilenet_v1'\n",
    "}\n",
    "\n",
    "# Chosing the model to use\n",
    "model_num = int(input(\"Enter a model number (1-7): \"))\n",
    "\n",
    "# Validating the model number\n",
    "if model_num in model_dict:\n",
    "    MODEL_NAME = model_dict[model_num]\n",
    "    print(\"Selected model:\", MODEL_NAME)\n",
    "else:\n",
    "    print(\"Invalid model number.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(ENV_PATH, 'models', MODEL_NAME)\n",
    "CKPT_PATH = os.path.join(MODEL_PATH, 'inference_graph/frozen_inference_graph.pb')\n",
    "LABEL_PATH = os.path.join(ENV_PATH, 'scripts', 'gtsdb3_label_map.pbtxt')\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Load a frozen Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.compat.v1.gfile.GFile(CKPT_PATH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Loading Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the label map\n",
    "label_map = label_map_util.load_labelmap(LABEL_PATH)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets a input as an image and returns it as a NumPy array.\n",
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function preprocesses the frames.\n",
    "def preprocess_frame(frame):\n",
    "\n",
    "    # Changing the color space from BGR to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Expanding the dimensions of the frame\n",
    "    frame_expanded = np.expand_dims(frame_rgb, axis=0)\n",
    "\n",
    "    return frame_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function overlays the bounding boxes, class labels and scores on the frame.\n",
    "def overlay(frame, boxes, classes, scores, box_color, box_thickness):\n",
    "    # Loop over all predictions\n",
    "    for i in range(boxes.shape[0]):\n",
    "        # Extract the bounding box coordinates\n",
    "        ymin, xmin, ymax, xmax = tuple(boxes[i].tolist())\n",
    "\n",
    "        # Extract the class label and score\n",
    "        class_label = category_index[int(classes[i])]\n",
    "        score = scores[i]\n",
    "\n",
    "        # Only draw the bounding box if the score is above 0.5\n",
    "        if np.squeeze(scores)[i] > 0.5:\n",
    "            # Draw the bounding box on the frame\n",
    "            cv2.rectangle(frame, (int(xmin*frame.shape[1]), int(ymin*frame.shape[0])),\n",
    "                        (int(xmax*frame.shape[1]), int(ymax*frame.shape[0])),\n",
    "                        box_color, box_thickness)\n",
    "\n",
    "            # Draw the class label and score on the frame\n",
    "            label = \"{}: {:.2f}\".format(class_label['name'], score)\n",
    "            label_color = (255, 255, 255) # White color\n",
    "            label_bg_color = (0, 255, 0) # Green color\n",
    "            label_thickness = 1\n",
    "            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, label_thickness)[0]\n",
    "            label_left = int(xmin*frame.shape[1])\n",
    "            label_top = int(ymin*frame.shape[0]) - label_size[1]\n",
    "            label_right = label_left + label_size[0]\n",
    "            label_bottom = label_top + label_size[1]\n",
    "            cv2.rectangle(frame, (label_left, label_top), (label_right, label_bottom), label_bg_color, -1)\n",
    "            cv2.putText(frame, label, (label_left, label_bottom), cv2.FONT_HERSHEY_SIMPLEX, 0.5, label_color, label_thickness)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EXTRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code outputs all the tensors in the graph.\n",
    "with detection_graph.as_default():\n",
    "    with tf.compat.v1.Session(graph=detection_graph) as sess:\n",
    "        for op in detection_graph.get_operations():\n",
    "            print(op.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Inference on Images**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to folder with images\n",
    "TEST_IMAGES_DIR_PATH = os.path.join(ENV_PATH, 'test_images')\n",
    "TEST_IMAGES_PATHS = glob.glob(os.path.join(TEST_IMAGES_DIR_PATH, '*.jpg'))\n",
    "\n",
    "# Size of the output images\n",
    "IMAGE_SIZE = (20, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    with tf.compat.v1.Session(graph=detection_graph) as sess:\n",
    "        for idx, image_path in enumerate(TEST_IMAGES_PATHS):\n",
    "            image = Image.open(image_path)\n",
    "            print(\"Processing image {}...\".format(image_path))\n",
    "            # the array based representation of the image will be used later in order to prepare the\n",
    "            # result image with boxes and labels on it.\n",
    "            image_np = load_image_into_numpy_array(image)\n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "            # Each box represents a part of the image where a particular object was detected.\n",
    "            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "            # Each score represent how level of confidence for each of the objects.\n",
    "            # Score is shown on the result image, together with the class label.\n",
    "            scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "            classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "            # Actual detection.\n",
    "            (boxes, scores, classes, num_detections) = sess.run(\n",
    "                [boxes, scores, classes, num_detections],\n",
    "                feed_dict={image_tensor: image_np_expanded})\n",
    "            \n",
    "            print(boxes,scores, classes, num_detections)\n",
    "            if np.squeeze(scores)[0] > 0.5:  # adjust the threshold as needed\n",
    "                # print the number and class of objects detected\n",
    "                num_objects = len(np.where(np.squeeze(scores) > 0.5)[0])\n",
    "                object_classes = [category_index[c]['name'] for c in np.squeeze(classes)[:num_objects]]\n",
    "                print(\"Detected {} objects: {}\".format(num_objects, ', '.join(object_classes)))\n",
    "                for i in range(num_objects):\n",
    "                    # create a new figure for each object detected\n",
    "                    plt.figure(idx*num_objects + i, figsize=IMAGE_SIZE)\n",
    "                    plt.axis('off')\n",
    "                    # crop and plot the image for the current object detected\n",
    "                    ymin, xmin, ymax, xmax = tuple(boxes[0][i].tolist())\n",
    "                    im_width, im_height = image.size\n",
    "                    (left, right, top, bottom) = (xmin * im_width, xmax * im_width, ymin * im_height, ymax * im_height)\n",
    "                    cropped_image = image.crop((left, top, right, bottom))\n",
    "                    plt.imshow(cropped_image)\n",
    "                    plt.show()\n",
    "                    \n",
    "            # Visualization of the results of a detection.\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np,\n",
    "                np.squeeze(boxes),\n",
    "                np.squeeze(classes).astype(np.int32),\n",
    "                np.squeeze(scores),\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                line_thickness=3)\n",
    "            # plot the original image\n",
    "            plt.figure(idx*num_objects + num_objects, figsize=IMAGE_SIZE)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(image_np)\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Inference on Video**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Defining Input and Output Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input video tensor\n",
    "frame_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "# Each box represents a part of the image where a particular object was detected.\n",
    "boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "# Each score represent how level of confidence for each of the objects.\n",
    "# Score is shown on the result image, together with the class label.\n",
    "scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "num_detections = detection_graph.get_tensor_by_name('num_detections:0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Converting the file from 30 fps to 5 fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATH_TO_VIDEO = '/Users/henriqueoliveira18/Documents/Developer/traffic-sign-detection/test_video/IMG_00245fps.mp4'\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(PATH_TO_VIDEO)\n",
    "\n",
    "# Get the input video frame rate and dimensions\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Create a VideoWriter object to write the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('/Users/henriqueoliveira18/Documents/Developer/traffic-sign-detection/test_video/VideoTest15fps.mp4', fourcc, 5, (frame_width, frame_height))\n",
    "\n",
    "# Initialize a frame counter and a frame skip factor\n",
    "frame_counter = 0\n",
    "frame_skip = int(frame_rate / 5)  # Skip frames to achieve 5 fps\n",
    "\n",
    "# Loop over the input video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the input video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # If the current frame is a multiple of the frame skip factor, write it to the output video\n",
    "    if frame_counter % frame_skip == 0:\n",
    "        out.write(frame)\n",
    "\n",
    "    # Increment the frame counter\n",
    "    frame_counter += 1\n",
    "\n",
    "# Release the input and output video objects\n",
    "cap.release()\n",
    "out.release()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Detection on video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a frame counter\n",
    "frames_counter = 0\n",
    "\n",
    "# Paths\n",
    "PATH_TO_VIDEO = '/Users/henriqueoliveira18/Documents/Developer/traffic-sign-detection/test_video/IMG_00245fps.mp4'\n",
    "PATH_TO_OUTPUT_VIDEO = '/Users/henriqueoliveira18/Documents/Developer/traffic-sign-detection/results_video/IMG_0024'+str(model_num)+'new'+'.mp4'\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(PATH_TO_VIDEO)\n",
    "\n",
    "# Get the frame dimensions\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Create a VideoWriter object to write the output video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(PATH_TO_OUTPUT_VIDEO, fourcc, frame_fps, (frame_width, frame_height))\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    with tf.compat.v1.Session(graph=detection_graph) as sess:\n",
    "        # Loop over all frames in the video\n",
    "        while cap.isOpened():\n",
    "            # Read the next frame\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            # If there are no more frames, break out of the loop\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Print the current frame number\n",
    "            frames_counter += 1\n",
    "            print(f\"Processing frame number: {frames_counter}/{num_frames}\")\n",
    "\n",
    "            # Preprocess the frame\n",
    "            preprocessed_frame = preprocess_frame(frame)\n",
    "            \n",
    "            # Perform inference on the frame\n",
    "            batch_boxes, batch_scores, batch_classes, batch_num_detections = sess.run(\n",
    "                [boxes, scores, classes, num_detections],\n",
    "                feed_dict={frame_tensor: preprocessed_frame})\n",
    "            \n",
    "            # Overlay the predictions onto the frame\n",
    "            overlayed_frame = overlay(frame, batch_boxes[0], batch_classes[0], batch_scores[0], (0,255,0), 5)\n",
    "\n",
    "            # Overlay the number of frames processed\n",
    "            cv2.rectangle(frame, (440, 40), (640, 100), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, str(frames_counter), (520, 90), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2)\n",
    "\n",
    "            # Display the detection on each frame\n",
    "            for i in range(batch_boxes[0].shape[0]):\n",
    "                if np.squeeze(batch_scores[0])[i] > 0.5:\n",
    "                    class_label = category_index[int(batch_classes[0][i])]\n",
    "                    score = batch_scores[0][i]\n",
    "                    label = \"{}: {:.2f}\".format(class_label['name'], score)\n",
    "                    print(label)\n",
    "\n",
    "            # Write out the frame to the output video file\n",
    "            out.write(overlayed_frame)\n",
    "\n",
    "            # Deleting all the variables\n",
    "            del batch_boxes, batch_scores, batch_classes, batch_num_detections\n",
    "\n",
    "        # Clean up\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TESTING AREA!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATH_TO_VIDEO = '/Users/henriqueoliveira18/Documents/Developer/traffic-sign-detection/test_video/VideoTest15fps.mp4'\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load a video file\n",
    "cap = cv2.VideoCapture(PATH_TO_VIDEO)\n",
    "\n",
    "# Read the first frame\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Printing the currente frame\n",
    "plt.figure(1)\n",
    "plt.axis('off')\n",
    "plt.imshow(frame)\n",
    "plt.show()\n",
    "\n",
    "# Preprocess the frame\n",
    "frame_processed = preprocess_frame(frame)\n",
    "\n",
    "# Printing the currente frame\n",
    "plt.figure(1)\n",
    "plt.axis('off')\n",
    "plt.imshow(frame_processed)\n",
    "plt.show()\n",
    "\n",
    "# Release the video capture object and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsr",
   "language": "python",
   "name": "tsr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
